<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="keywords" content="Amitabh Yadav, Home Page, Amitabh Yadav Website, Amitabh Yadav Portfolio, Amitabh Yadav CERN, Amitabh Yadav BARC, Amitabh Yadav NASA, Amitabh Yadav Profile, Amitabh Yadav Lockheed Martin, Amitabh Yadav Blog, Amitabh Yadav TU Delft, Amitabh Yadav UPES, Amitabh Yadav Lucknow, Amitabh Yadav India, Amitabh Yadav Engineer, Amitabh Yadav Electronics, Amitabh Yadav Quantum" />
    <meta name="description" content="Electronics and Computer Engineer" />
    <meta name="author" content="Amitabh Yadav">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" type="image/png" href="../../../assets/icons/favicon.png"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <title>Amitabh's Notes / An outlook on Agile Hardware Development Methodology for accelerating full-stack development of Deep Learning SoCs</title> <!-- Update Title -->
    <link rel="stylesheet" href="../../../assets/style.css">
    <!-- for mathjax -->
    <script>
        MathJax = {
            loader: {
             load: [
              'input/tex-base', '[tex]/newcommand', '[tex]/action',
              'output/chtml'
             ]
            },
            tex: {
             tags: "all", // should be 'ams', 'none', or 'all'
             inlineMath: [['$', '$'], ['\\(', '\\)']],
             packages: ['base', 'newcommand', 'action']
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
    <!-- for code and syntax highlighting -->
    <!-- Highlight.js Library -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.4.0/highlight.min.js"></script>
    <!-- Disqus Comments -->
    <script src="//amitabhydv.disqus.com/embed.js" async></script>
    <!-- jquery -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" type="text/javascript"></script>
    <!-- Google Analytics starts here -->
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MJFK7Q82Z8"></script   >
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-MJFK7Q82Z8');
    </script>
    <!-- Google Analytics ends here -->
</head>
<body>
<header class="site-header">
    <div class="header-content">
        <div class="site-name"><a href="../../../" style="text-decoration: none; color:inherit;">Amitabh's Notes</a></div>
        <span class="print-only" style="display: none;">&#169; 2023 Amitabh Yadav</span>
        <div class="share-icons no-print">
            <!-- Icons can be implemented using images, font icons like Font Awesome, or SVGs -->
            <!--<a onclick="" id="subscribe-button" class="no-print">Subscribe</a>-->
            <a href="../../../"><span style="color: #159BD6;">[ ARTICLES ]</span></a>
            <!--<a href="../../../books">BOOKS</a>
            <a href="../../../videos/">VIDEOS</a>
            <a href="../../../photographs/">PHOTOGRAPHS</a> -->
            <a href="https://www.linkedin.com/in/amitabhx"><img src="../../../assets/icons/linkedin-icon.svg" alt="LinkedIn" style="position: relative; top: 4px;"></a>
            <a href="https://www.x.com/squareonehouse"><img src="../../../assets/icons/x-social-icon.svg" alt="Twitter" style="position: relative; top: 4px; left: -10px;"></a>
            <a href="https://www.instagram.com/squareone.house"><img src="../../../assets/icons/instagram-icon.svg" alt="Instagram" style="position: relative; top: 4px; left: -21px;"></a>
            <a href="https://www.youtube.com/@squareonehouse"><img src="../../../assets/icons/youtube-icon.svg" alt="Twitter" style="position: relative; top: 4px; left: -32px;"></a>
            
            <!-- Add more icons as needed -->
        </div>
    </div>
</header>

</br>

<div class="article-container">
<div class="all-articles">
<div class="article" data-category="Technology" data-tags="Research">
    <h1 class="article-title">An outlook on Agile Hardware Development Methodology for accelerating full-stack development of Deep Learning SoCs</h1> <!-- Update Title -->
    <h4>In modern Deep Learning accelerators, the computer architecture used to train a deep learning model affects the model’s accuracy, speed and even its ability to generalise to new data. This creates a massive multi-dimensional design space to find pareto-optimal configuration per use-case. Additionally, new ISAs need new software and compilers. How do we navigate this maze of infinite possibilities? What is Agile development for hardware all about?</h4> <!-- Update Description -->
    <h5>February 03, 2023 &nbsp;&#x2022;&nbsp; <span id="reading-time"></span></h5> <!-- Update Date -->
        <div class="image-caption-container">
            <img src="assets/images/agile-methodology.webp" width="80%"></center></br> <!-- Update Header Image -->
        <!--<div class="img-caption">Agile Development Methodology</div> -->
    </div>
    <article>
    <p> 
    <!-- Start Writing Your Article here-->
    <p><span style="color:#159BD6;">1. Future Computing Systems: Where are we heading post-Moore era?</span></br></br>
In the post Moore’s Law era, parallelism isn’t going to be the boat for us to set sail for the next generation of computing systems; power is going to be the principal factor dictating the research direction for heterogeneous accelerators. The architectural challenge is to maintain a balance between performance and functional flexibility; and the key QoR metrics to report are computational capability (GOps) and power efficiency (GOps/W), overall computational effectiveness (precision) and full-stack integration. This is highlighted in the AI and ML accelerator trends (Figure 1).</br></br>

<div class="image-caption-container">
        <img src="assets/images/ai_accelerators_2022.png">
    <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div>
</div>

In modern DNN accelerators, the architecture used to train a deep learning model affects the model’s accuracy, speed and even its ability to generalise to new data. This creates a massive multi-dimensional design space to find pareto-optimal configuration per use-case. Typically, a new reliable architecture design from ideation to tape-out takes three to four years by a team - which besides time, incurs a huge development cost (multi-Million $). This motivates us to explore an ASIC development methodology that should allow application experts to configure the lower layers of the computing stack to create these new efficient systems through high-level synthesis, and design reuse through rich parameterization and incremental extensions. This disparages the Waterfall model that has been adopted since the inception of integrated circuits to an Agile model for continuous development and integration for DNN accelerators.</br></br>

In this short article, we discuss two of the Agile Hardware-Software Co-design Frameworks, namely, Agile Hardware (AHA) initiative at Stanford Accelerate Lab and Chipyard Framework developed at UC Berkeley’s ADEPT (Agile Design of Efficient Processing Technologies) Lab (now, SLICE lab), with emphasis on their respective accelerator architectures and implemented optimizations for accelerating deep neural networks. In section 2, we discuss in detail the Gemmini accelerator and in section 3 we dive into the details of Coarse-Grained Reconfigurable Array (CGRA). Finally, in section 4, we compare and contrast the two approaches, identify their strengths and weaknesses, and present our conclusions.</br></br>

<span style="color:#159BD6;">2. Chipyard and Gemmini</span></br></br>
Chipyard + Gemmini are one of the more mature open-source frameworks for custom SoC-accelerator generators (aside from industry, e.g. ASIP designer) that adopts an agile approach. Chipyard is a complete suite for physically realisable SoC design, simulation, validation and full-stack integration to RISC-V and BOOM cores via RoCC port of TileLink to enable configurable generator-based accelerators (such as Gemmini) IP blocks that support rich parameterization and RISC-V ISA extensions. The number of tools are extensive and it would be unjust to briefly summarise. Figure below shows Chipyard workflow and Gemmini integration and attempts to share a glimpse.</br></br>

<div class="image-caption-container">
        <img src="assets/images/chipyard.png">
    <!-- <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div> -->
</div>

Gemmini is a matrix multiplication accelerator containing:</br>
<ul>
<li>2-tier organised Systolic array for flexible microarchitecture: pipelined tiles, containing PEs (not pipelined) for dot product and accumulation. This enables both systolic spatial array (TPU) and parallel vector engines (NVDLA).</li>
<li>Connects to RocketCore/BOOM via RoCC; by default maps to L2 shared cache.</li>
<li>Main memory connects host to Gemmini’s Scratchpad (SRAM) and Accumulator. DMA access keeps latency low.</li>
<li>Scratchpad stores the inputs, and accumulator stores partial sums and final results.</li>
<li>Computation happens in weight stationary or output stationary dataflow, which is configurable at run-time/hardened in silicon.</li>
<div class="image-caption-container">
        <img src="assets/images/ow-stationary.png">
    <!-- <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div> -->
</div>
<li>optionally peripheral circuitry for activation functions ReLU/ReLU6, result precision scaling by powers-of-2 for quantization, and pre-processing e.g. transpose matrix circuit before directing to the systolic array.</li>
<li>ONNX/C for programming.</li>
<li>Heuristic algorithms are used for data staging and mapping.</li>
</li>
</ul>

Systolic Array configs:</br>
<ul>
<li>Systolic array dimensions (tileRows, tileColumns, meshRows, meshColumns)</li>
<li>Input Stationary/Weight Stationary (dataflow)</li>
<li>Scratchpad/Accumulator capacity (in Kib) (sp_banks, sp_capacity, acc_capacity)</li>
<li>Datatypes for I/O and partial accumulations (inputType, outputType, accType)</li>
<li>Access-execute decoupling queue parameters (ld ,st, ex, rob)</li>
<li>DMA parameters and scaling factor (optional).</li>
<li>Custom RISC-V ISA extension support.</li>
</ul>
<div class="image-caption-container">
        <img src="assets/images/cnnhw.png">
    <!-- <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div> -->
</div>
Or, can be achieved via GEMM.</br></br>

Image processing data –> im2col –> Matrix Multiplication</br></br>

(Needs convolution)    (preprocessing)   (Nested for loops)</br></br>

Performance:</br>

<div class="image-caption-container">
        <img src="assets/images/nn-gemmini-perf.png">
    <!-- <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div> -->
</div>
Memory partitioning strategy, tiling and scheduling can greatly impact performance of workload. Profiling access patterns in workloads can give better ideas in dataflow optimization. Spatial and Temporal data-reuse can further impact performance. Maximum area (67.1%) is still captured by SRAM on chip, followed by the rocket core and then the accelerator.</br></br>

In conclusion, what makes Gemmini unique from other implementations is the manufacturability as a full SoC, Linux OS support and Virtual address translation support (along with shared memory access). Dataflow, Bitwidth, Input Dimensions, Bus Size, HOST CPU,Pipelines, Memory and memory banks all affect the performance. Gemmini enables exploration of a vast design space, profile system-level performance using Chipyard + FireMarshal infrastructure and uncover reliability issues that can occur across the stack.</br></br>

<span style="color:#159BD6;">3. Agile Hardware (AHA) of Amber SoC using CGRA</span></br></br>
A Coarse Grain Reconfigurable Architecture (CGRA) is a reconfigurable architecture that operates on coarser granularity than traditional reconfigurable architectures such as FPGA (fine-grained). CGRA consists of an array of Processing Elements (PE) and memory (MEM) tiles connected via reconfigurable interconnect that are capable of executing word-level operations and can be reconfigured to minimise idling time through online partial reconfiguration (necessary of edge devices and servers running parallel applications within the fabric).</br></br>

<div class="image-caption-container">
        <img src="assets/images/aha-stanford.png">
    <!-- <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div> -->
</div>
Each hardware block has its own DSL for PE (PEak), MEM (Lake) and Interconnect (Canal). This is compiled to generate the final RTL and routed to the VLSI flow to generate GDSII. The compilers map applications written in Halide to a CGRA bitstream. It receives updated specifications (rewrite rules, routing graph) from the DSL-based hardware generators, allowing it to compile to evolving CGRA hardware with no manual updates. The coupled accelerator compiler takes the rewrite rules generated by each DSL compiler to map applications to the hardware. Halide (DSL in C/C++) specified applications are mapped to CGRA (Figure 5).</br></br>

Amber SoC is a 367 GOPS, 538 GOPS/W 16nm SoC designed with the AHA co-design approach for dense linear algebra applications (Figure 4).</br></br>

<ul>
<li>Amber demonstrates 2x speed up compared to baseline FPGA by reduction in idle time through fast (3.5us) dynamic partial reconfiguration (DPR).</li>

<div class="image-caption-container">
        <img src="assets/images/aha-perf.png">
    <!-- <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div> -->
</div>
<li>It intelligently handles data access to streaming memory without DMA access (to reduce power consumption) by leveraging the affine data access patterns (e.g. nested for loop) in dense linear algebra applications. This is done using Iteration Domain (range of memory identification), Address Generator (address calculation) and Schedule generator (read/write signals). No manual intervention is necessary as the application compiler handles this task.</li>
<li>Wide fetch SRAM in streaming memory to reduce energy consumption (48%), and Resource sharing of ID/AD/SG to reduce area (50%).</li>
<li>It chains together PEs to perform less frequent but necessary algebraic operations (FPdiv, log, sin etc) with very small area overhead (0.2%) and saves 37.8% area compared to on ship complex op circuitry.</li>
</ul>

<span style="color:#159BD6;">4. Conclusions</span></br>
<ul>
<li>The cost of verification and software maintainability (of both, existing and new codebase) would be the key to determine the way forwards in computing.</li>
<div class="image-caption-container">
        <img src="assets/images/chip-design-cost.png">
    <!-- <div class="img-caption">Figure 1: Peak performance vs peak power of publically announced accelerators in 2022 <a href="https://www.arxiv.org/abs/2210.04055" target="_blank" style="color:#159BD6;">[<u>arxiv:2210.04055</u>]</a></div> -->
    </div>
<li>Both face a similar challenge in developing efficient algorithms for mapping data/software programs onto the systolic arrays/CGRA. This can be difficult, especially for large and complex applications. In contrast to systolic arrays, CGRAs are flexible enough to adapt to different applications.</li>
<li>Both, Systolic Arrays and CGRAs may not perform as well as specialised hardware for certain types of tasks. For example, a general-purpose processor may be faster for tasks that require a high level of sequential processing. In this sense, both are domain-specific architectures.</li>
<li>In either case, a rigorous approach to system-level verification and validation is a challenge; for which formal methods, DSE tools and profiling tools would be necessary. Further, Physical design needs manual interventions, which can drastically impact performance. This can be mitigated by automated tests for DFM (Design for Manufacturability/Sign-Off).</li>
</ul>

To conclude, the fragmented landscape of ASICs for computing are causing a barrier to new innovations from the software frontend. Standardising the practices for developing new ASICs or CGRAs is a direction in the right step. What can be improved here is to allow for more comfortable entry points to SOC practitioners to allow configuration and testing of these devices through a cloud based infrastructure to iteratively evolve and make the portability of the existing software to the hardware feasible.</br></br>

Lastly, on a personal observation, the codebase of both needs a lot more comments. It’s a mess.
</p>
    <!-- All text in the article must end here -->
    </p>
</article><br>
<a href="" class="comment-btn-click">Show Comments</a>
<div id="disqus_thread" class="no-print comment-block"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
        this.page.url = "http://localhost:4000/posts/2023/read-your-pdf-better/";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "http://localhost:4000/posts/2023/read-your-pdf-better/"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://amitabhydv.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments.</a></noscript>
                
</div></div></div>



<!-- Footer Stuff -->
<button onclick="topFunction()" id="topbtn" title="Go to top" class="no-print" style="display: block;"><img src="../../../assets/icons/top-icon.svg" width="25px"></button>

<div class="theme-toggle-container no-print">
<div class="theme-switch">
    <label id="switch" class="switch">
        <input type="checkbox" id="theme-toggle">
        <span class="slider round"></span>
    </label>
</div>
</div>
<img class="no-print no-show" src="../../../assets/mainImages/amitabhsnotes.png" style="position: fixed; bottom:-10px; right:0%; width: 160px; display: none;" >
<span class="no-print no-show" style="position: fixed; bottom:0%; right:0%; display: none; color:lightgrey;">&#169; 2023 Amitabh Yadav</span>
<script>hljs.highlightAll();</script>
<script src="../../../assets/script.js"></script><!-- used for dark/light theme transition and reading time calculation-->
<!-- Scroll to Top Script -->
<script>
//Get the button
var mybutton = document.getElementById("topbtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>
<!-- Comment block show/hide -->
<script type="text/javascript">
        var $clickme = $('.comment-btn-click'),
    $box = $('.box');
        //$box.hide();
        $clickme.click(function (e) {
            $(this).text(($(this).text() === 'Show Comments' ? 'Hide Comments' : 'Show Comments')).next('.comment-block').slideToggle(); ;
            e.preventDefault();
        });
</script>
</body>
</html>
